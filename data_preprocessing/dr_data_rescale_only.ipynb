{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Dataset citation]\n",
    "##### <EyePACS dataset 2015>\n",
    "- Emma Dugas, Jared, Jorge, Will Cukierski. (2015). Diabetic Retinopathy Detection. Kaggle. https://kaggle.com/competitions/diabetic-retinopathy-detection\n",
    "\n",
    "##### <APTOS 2019>\n",
    "- Karthik, Maggie, Sohier Dane. (2019). APTOS 2019 Blindness Detection. Kaggle. https://kaggle.com/competitions/aptos2019-blindness-detection\n",
    "\n",
    "##### <INDIAN DIABETIC RETINOPATHY IMAGE DATASET (IDRID)>\n",
    "- Prasanna Porwal, Samiksha Pachade, Ravi Kamble, Manesh Kokare, Girish Deshmukh, Vivek Sahasrabuddhe, Fabrice Meriaudeau, April 24, 2018, \"Indian Diabetic Retinopathy Image Dataset (IDRiD)\", IEEE Dataport, doi: https://dx.doi.org/10.21227/H25W98.\n",
    "- https://www.kaggle.com/datasets/gami4388/diabetic-retinopathy-resized-train-15-19-dg?select=resized_train_15_19_DG\n",
    "\n",
    "##### <Messidor 2>\n",
    "- Abramoff et al, Automated analysis of retinal images for detection of referable diabetic retinopathy, JAMA Ophthalmol. 2013;131:351-7, and in Abramoff et al, Improved automated detection of diabetic retinopathy on a publicly available dataset through integration of deep learning, IOVS. 57:5200-06.\n",
    "- https://medicine.uiowa.edu/eye/abramoff\n",
    "- https://www.kaggle.com/datasets/mariaherrerot/messidor2preprocess/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Package load]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# 이걸 해줘야 matplotlib 시행 시 에러가 안 남\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20051020_45050_0100_PP</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051020_54209_0100_PP</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051020_57761_0100_PP</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051216_45226_0200_PP</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051216_47000_0200_PP</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26067_right</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36551_left</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25142_right</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5124_right</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589_left</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40507 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        diagnosis\n",
       "20051020_45050_0100_PP          3\n",
       "20051020_54209_0100_PP          3\n",
       "20051020_57761_0100_PP          3\n",
       "20051216_45226_0200_PP          3\n",
       "20051216_47000_0200_PP          3\n",
       "...                           ...\n",
       "26067_right                     0\n",
       "36551_left                      2\n",
       "25142_right                     2\n",
       "5124_right                      0\n",
       "14589_left                      2\n",
       "\n",
       "[40507 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data_dir = '.\\original data'\n",
    "original_test_csv = pd.read_csv(os.path.join('.','test_label.csv'), index_col=0)\n",
    "original_train_csv = pd.read_csv(os.path.join('.','train_label_original.csv'), index_col=0)\n",
    "original_val_csv = pd.read_csv(os.path.join('.','val_label.csv'), index_col=0)\n",
    "\n",
    "concat_csv = pd.concat([original_test_csv, original_train_csv, original_val_csv])\n",
    "concat_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24304\n",
      "8102\n",
      "8101\n"
     ]
    }
   ],
   "source": [
    "train_csv = concat_csv.sample(frac=0.6, replace=False)\n",
    "train_id = train_csv.index.to_list()\n",
    "concat_csv = concat_csv.drop(train_id)\n",
    "val_csv = concat_csv.sample(frac=0.5, replace=False)\n",
    "val_id = val_csv.index.to_list()\n",
    "test_csv = concat_csv.drop(val_id)\n",
    "\n",
    "print(len(train_csv))\n",
    "print(len(val_csv))\n",
    "print(len(test_csv))\n",
    "\n",
    "train_csv.to_csv(os.path.join('.', 'train_label_rescale.csv'), index=True)\n",
    "val_csv.to_csv(os.path.join('.', 'val_label_rescale.csv'), index=True)\n",
    "test_csv.to_csv(os.path.join('.', 'test_label_rescale.csv'), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Image Rescale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def img_crop(img):\n",
    "    # 대각선만큼 상하좌우 패딩\n",
    "    img_h, img_w = img.shape[0], img.shape[1]\n",
    "    diag = int((img_h**2 + img_w ** 2)**0.5)    # 대각선 길이 구하기\n",
    "    add_h, add_w = int((diag - img_h)/2), int((diag - img_w)/2)     # 상하좌우에 각각 추가될 padding의 길이 구하기\n",
    "    img = cv2.copyMakeBorder(img, add_h, add_h, add_w, add_w, cv2.BORDER_CONSTANT,value=0)  # 0이라는 CONSTANT (검은색)으로 가장자리 추가\n",
    "\n",
    "    # actan만큼 회전\n",
    "    img_h, img_w = img.shape[0], img.shape[1]\n",
    "    degree = math.degrees(math.atan(img_h / img_w)) # actan에 해당하는 각도 구하기\n",
    "    x_center, y_center = int(img_h/2), int(img_w/2) # 이미지 중심의 좌표 구하기\n",
    "    matrix = cv2.getRotationMatrix2D((x_center, y_center), -degree, 1)  # 중심을 기준으로 해서 시계방향으로 degree만큼 (-degree) 회전하는 행렬 구하기, scal (확대 비율)은 그대로 1\n",
    "    img = cv2.warpAffine(img, matrix, (diag, diag))   # 구한 rotation matrix를 img에 적용\n",
    "    \n",
    "    # Crop하기\n",
    "    img = cv2.copyMakeBorder(img, 10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])    # 회전시킨 영상에서 다시 상하좌우 10만큼 검은색 배경 추가 -> border를 잘 자르기 위해서\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, gray = cv2.threshold(gray, 5, 255, cv2.THRESH_BINARY)    # 밝기 5 이하를 0, 그 이상은 255로 처리 -> 배경을 싹다 0으로 만들어주기\n",
    "    contours,hierarchy = cv2.findContours(gray,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # 여기서 gray를 binary로 넣어야 해서 gray로 시작하는 image 전처리가 필요함\n",
    "        # cv2.RETR_EXTERNAL: 가장 바깥쪽 라인만 생성\n",
    "        # cv2.CHAIN_APPROX_SIMPLE: 컨투어 꼭짓점 좌표만 제공 -> 가장 바깥쪽 원의 좌표만 제공\n",
    "    contours = max(contours, key=cv2.contourArea)\n",
    "    x,y,w,h = cv2.boundingRect(contours)\n",
    "        # 주어진 점을 감싸는 최소 크기 사각형(바운딩 박스)를 반환 -> x,y,w,h는 bounding box에 대한 좌표\n",
    "    img = img[y:y+h, x:x+w]\n",
    "    img_H, img_W = img.shape[0], img.shape[1]\n",
    "    ret = max(img_H, img_W)\n",
    "\n",
    "    # 반대 rotation\n",
    "    img_H, img_W = img.shape[0], img.shape[1]\n",
    "    x_center, y_center = int(img_H/2), int(img_W/2)\n",
    "    matrix = cv2.getRotationMatrix2D((x_center, y_center), degree, 1)   # 반대로 rotation\n",
    "    img = cv2.warpAffine(img, matrix, (img_H, img_W))\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _,gray = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "    contours,hierarchy = cv2.findContours(gray,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = max(contours, key=cv2.contourArea)\n",
    "    x,y,w,h = cv2.boundingRect(contours)\n",
    "    img = img[y:y+h, x:x+w]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_resize(img, mode, size):\n",
    "    if size == 1024:\n",
    "        train_size = 1200\n",
    "        val_test_size = 1024\n",
    "    elif size == 512:\n",
    "        train_size = 600\n",
    "        val_test_size = 512\n",
    "    if mode == 'train':\n",
    "        if min(img.shape[0], img.shape[1]) <= train_size: # 이미지 size가 1600보다 작아서 확대해야 하는 경우\n",
    "            img = cv2.resize(img, dsize=(train_size,train_size), interpolation=cv2.INTER_CUBIC)\n",
    "        else:   # 이미지 size가 1024보다 커서 축소 해야 하는 경우\n",
    "            img = cv2.resize(img, dsize=(train_size,train_size), interpolation=cv2.INTER_AREA)\n",
    "    elif mode == 'val':\n",
    "        if min(img.shape[0], img.shape[1]) <= val_test_size: # 이미지 size가 1024보다 작아서 확대해야 하는 경우\n",
    "            img = cv2.resize(img, dsize=(val_test_size,val_test_size), interpolation=cv2.INTER_CUBIC)\n",
    "        else:   # 이미지 size가 1024보다 커서 축소 해야 하는 경우\n",
    "            img = cv2.resize(img, dsize=(val_test_size,val_test_size), interpolation=cv2.INTER_AREA)\n",
    "    elif mode == 'test':\n",
    "        if min(img.shape[0], img.shape[1]) <= val_test_size: # 이미지 size가 1024보다 작아서 확대해야 하는 경우\n",
    "            img = cv2.resize(img, dsize=(val_test_size,val_test_size), interpolation=cv2.INTER_CUBIC)\n",
    "        else:   # 이미지 size가 1024보다 커서 축소 해야 하는 경우\n",
    "            img = cv2.resize(img, dsize=(val_test_size,val_test_size), interpolation=cv2.INTER_AREA)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '.'\n",
    "\n",
    "def rescale_and_save(mode, path_list, size):\n",
    "    print('start preprocessing...')\n",
    "    completed_list = sorted(glob.glob(os.path.join(data_dir, 'rescale_512', str(mode), '*')))\n",
    "    completed_list_name = [os.path.split(x)[-1] for x in completed_list]\n",
    "    working_list = [x for x in path_list if os.path.split(x)[-1] not in completed_list_name]\n",
    "    for img_path in tqdm(working_list):\n",
    "        img_name = os.path.split(img_path)[-1]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = img_crop(img)\n",
    "        img = img_resize(img, mode=mode, size = size)\n",
    "        cv2.imwrite(os.path.join(data_dir, 'rescale_512', str(mode), img_name), img)\n",
    "    print('finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print(len(train_csv)):  24304\n",
      "len(train_path):  24304\n",
      "print(len(val_csv)):  8102\n",
      "len(val_path):  8102\n",
      "print(len(test_csv)):  8101\n",
      "len(test_path):  8101\n"
     ]
    }
   ],
   "source": [
    "total_file_path = sorted(glob.glob(os.path.join(original_data_dir, 'sorted','*', '*')))\n",
    "\n",
    "train_csv = pd.read_csv(os.path.join(data_dir,'train_label_rescale.csv'), index_col=0)\n",
    "train_id = train_csv.index.to_list()\n",
    "train_path = [x for x in total_file_path if os.path.basename(x).split('.')[0] in train_id]\n",
    "print(\"print(len(train_csv)): \", len(train_csv))\n",
    "print(\"len(train_path): \", len(train_path))\n",
    "\n",
    "val_csv = pd.read_csv(os.path.join(data_dir,'val_label_rescale.csv'), index_col=0)\n",
    "val_id = val_csv.index.to_list()\n",
    "val_path = [x for x in total_file_path if os.path.basename(x).split('.')[0] in val_id]\n",
    "print(\"print(len(val_csv)): \", len(val_csv))\n",
    "print(\"len(val_path): \", len(val_path))\n",
    "\n",
    "test_csv = pd.read_csv(os.path.join(data_dir,'test_label_rescale.csv'), index_col=0)\n",
    "test_id = test_csv.index.to_list()\n",
    "test_path = [x for x in total_file_path if os.path.basename(x).split('.')[0] in test_id]\n",
    "print(\"print(len(test_csv)): \", len(test_csv))\n",
    "print(\"len(test_path): \", len(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7d81dbc85a4587aa685ead5fa1735f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish!\n"
     ]
    }
   ],
   "source": [
    "rescale_and_save('test', test_path, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79b8feed8d14ecba513bf40ea3c5ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish!\n"
     ]
    }
   ],
   "source": [
    "rescale_and_save('val', val_path, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1d3af934764cf8940d12edc6f7d7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish!\n"
     ]
    }
   ],
   "source": [
    "rescale_and_save('train', train_path, 512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
